{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMsUDkZNg/Nzt0774XCEWSq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bghdd/Forward-Forward/blob/main/forward_forward_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import random\n",
        "from datetime import timedelta\n",
        "import torchvision\n",
        "import time\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "w427iVWWMjS7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt_seed =42\n",
        "device=\"cuda\"  # cpu or cuda\n",
        "\n",
        "input_path= './datasets'\n",
        "input_batch_size= 100\n",
        "\n",
        "\n",
        "model_peer_normalization= 0.03\n",
        "model_momentum= 0.9  # Momentum to use for the running mean in peer normalization loss.\n",
        "\n",
        "model_hidden_dim= 1000\n",
        "model_num_layers= 3\n",
        "\n",
        "\n",
        "training_epochs= 100\n",
        "\n",
        "training_learning_rate= 1e-3\n",
        "training_weight_decay= 3e-4\n",
        "training_momentum= 0.9\n",
        "\n",
        "training_downstream_learning_rate= 1e-2\n",
        "training_downstream_weight_decay= 3e-3\n",
        "\n",
        "training_val_idx= -1  # -1: validate only once training has finished; n: validate every n epochs.\n",
        "training_final_test= True  # Set to true to evaluate performance on test-set.\n"
      ],
      "metadata": {
        "id": "u3KIwT-rPfN2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "make the mnist dataset suitable for FF"
      ],
      "metadata": {
        "id": "btiRI7dCL1dM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FF_MNIST(torch.utils.data.Dataset):\n",
        "    def __init__(self, partition, num_classes=10):\n",
        "        self.mnist = get_MNIST_partition(partition)\n",
        "        self.num_classes = num_classes\n",
        "        self.uniform_label = torch.ones(self.num_classes) / self.num_classes\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        pos_sample, neg_sample, neutral_sample, class_label = self._generate_sample(\n",
        "            index\n",
        "        )\n",
        "\n",
        "        inputs = {\n",
        "            \"pos_images\": pos_sample,\n",
        "            \"neg_images\": neg_sample,\n",
        "            \"neutral_sample\": neutral_sample,\n",
        "        }\n",
        "        labels = {\"class_labels\": class_label}\n",
        "        return inputs, labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.mnist)\n",
        "\n",
        "    def _get_pos_sample(self, sample, class_label):\n",
        "        one_hot_label = torch.nn.functional.one_hot(\n",
        "            torch.tensor(class_label), num_classes=self.num_classes\n",
        "        )\n",
        "        pos_sample = sample.clone()\n",
        "        pos_sample[:, 0, : self.num_classes] = one_hot_label\n",
        "        return pos_sample\n",
        "\n",
        "    def _get_neg_sample(self, sample, class_label):\n",
        "        # Create randomly sampled one-hot label.\n",
        "        classes = list(range(self.num_classes))\n",
        "        classes.remove(class_label)  # Remove true label from possible choices.\n",
        "        wrong_class_label = np.random.choice(classes)\n",
        "        one_hot_label = torch.nn.functional.one_hot(\n",
        "            torch.tensor(wrong_class_label), num_classes=self.num_classes\n",
        "        )\n",
        "        neg_sample = sample.clone()\n",
        "        neg_sample[:, 0, : self.num_classes] = one_hot_label\n",
        "        return neg_sample\n",
        "\n",
        "    def _get_neutral_sample(self, z):\n",
        "        z[:, 0, : self.num_classes] = self.uniform_label\n",
        "        return z\n",
        "\n",
        "    def _generate_sample(self, index):\n",
        "        # Get MNIST sample.\n",
        "        sample, class_label = self.mnist[index]\n",
        "        pos_sample = self._get_pos_sample(sample, class_label)\n",
        "        neg_sample = self._get_neg_sample(sample, class_label)\n",
        "        neutral_sample = self._get_neutral_sample(sample)\n",
        "        return pos_sample, neg_sample, neutral_sample, class_label"
      ],
      "metadata": {
        "id": "rEVY4jw73RLk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "yN16eynwLyrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FF_model(torch.nn.Module):\n",
        "    \"\"\"The model trained with Forward-Forward (FF).\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(FF_model, self).__init__()\n",
        "        self.input_batch_size=input_batch_size\n",
        "        self.model_num_layers= model_num_layers\n",
        "        self.device = device\n",
        "        self.model_momentum = model_momentum\n",
        "        self.model_peer_normalization = model_peer_normalization\n",
        "        self.num_channels = [model_hidden_dim] * self.model_num_layers\n",
        "        self.act_fn = ReLU_full_grad()\n",
        "        # Initialize the model.\n",
        "        self.model = nn.ModuleList([nn.Linear(784, self.num_channels[0])])\n",
        "        for i in range(1, len(self.num_channels)):\n",
        "            self.model.append(nn.Linear(self.num_channels[i - 1], self.num_channels[i]))\n",
        "\n",
        "        # Initialize forward-forward loss.\n",
        "        self.ff_loss = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        # Initialize peer normalization loss.\n",
        "        self.running_means = [\n",
        "            torch.zeros(self.num_channels[i], device=self.device) + 0.5\n",
        "            for i in range(self.model_num_layers)\n",
        "        ]\n",
        "\n",
        "        # Initialize downstream classification loss.\n",
        "        channels_for_classification_loss = sum(\n",
        "            self.num_channels[-i] for i in range(self.model_num_layers - 1)\n",
        "        )\n",
        "        self.linear_classifier = nn.Sequential(\n",
        "            nn.Linear(channels_for_classification_loss, 10, bias=False)\n",
        "        )\n",
        "        self.classification_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Initialize weights.\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.model.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                torch.nn.init.normal_(\n",
        "                    m.weight, mean=0, std=1 / math.sqrt(m.weight.shape[0])\n",
        "                )\n",
        "                torch.nn.init.zeros_(m.bias)\n",
        "\n",
        "        for m in self.linear_classifier.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.zeros_(m.weight)\n",
        "\n",
        "    def _layer_norm(self, z, eps=1e-8):\n",
        "        return z / (torch.sqrt(torch.mean(z ** 2, dim=-1, keepdim=True)) + eps)\n",
        "\n",
        "    def _calc_peer_normalization_loss(self, idx, z):\n",
        "        # Only calculate mean activity over positive samples.\n",
        "        mean_activity = torch.mean(z[:self.input_batch_size], dim=0)\n",
        "\n",
        "        self.running_means[idx] = self.running_means[\n",
        "            idx\n",
        "        ].detach() * self.model_momentum + mean_activity * (\n",
        "            1 - self.model_momentum\n",
        "        )\n",
        "\n",
        "        peer_loss = (torch.mean(self.running_means[idx]) - self.running_means[idx]) ** 2\n",
        "        return torch.mean(peer_loss)\n",
        "\n",
        "    def _calc_ff_loss(self, z, labels):\n",
        "        sum_of_squares = torch.sum(z ** 2, dim=-1)\n",
        "\n",
        "        logits = sum_of_squares - z.shape[1]\n",
        "        ff_loss = self.ff_loss(logits, labels.float())\n",
        "\n",
        "        with torch.no_grad():\n",
        "            ff_accuracy = (\n",
        "                torch.sum((torch.sigmoid(logits) > 0.5) == labels)\n",
        "                / z.shape[0]\n",
        "            ).item()\n",
        "        return ff_loss, ff_accuracy\n",
        "\n",
        "    def forward(self, inputs, labels):\n",
        "        scalar_outputs = {\n",
        "            \"Loss\": torch.zeros(1, device=self.device),\n",
        "            \"Peer Normalization\": torch.zeros(1, device=self.device),\n",
        "        }\n",
        "\n",
        "        # Concatenate positive and negative samples and create corresponding labels.\n",
        "        z = torch.cat([inputs[\"pos_images\"], inputs[\"neg_images\"]], dim=0)\n",
        "        posneg_labels = torch.zeros(z.shape[0], device=self.device)\n",
        "        posneg_labels[: self.input_batch_size] = 1\n",
        "\n",
        "        z = z.reshape(z.shape[0], -1)\n",
        "        z = self._layer_norm(z)\n",
        "\n",
        "        for idx, layer in enumerate(self.model):\n",
        "            z = layer(z)\n",
        "            z = self.act_fn.apply(z)\n",
        "\n",
        "            if self.model_peer_normalization > 0:\n",
        "                peer_loss = self._calc_peer_normalization_loss(idx, z)\n",
        "                scalar_outputs[\"Peer Normalization\"] += peer_loss\n",
        "                scalar_outputs[\"Loss\"] += self.model_peer_normalization * peer_loss\n",
        "\n",
        "            ff_loss, ff_accuracy = self._calc_ff_loss(z, posneg_labels)\n",
        "            scalar_outputs[f\"loss_layer_{idx}\"] = ff_loss\n",
        "            scalar_outputs[f\"ff_accuracy_layer_{idx}\"] = ff_accuracy\n",
        "            scalar_outputs[\"Loss\"] += ff_loss\n",
        "            z = z.detach()\n",
        "\n",
        "            z = self._layer_norm(z)\n",
        "\n",
        "        scalar_outputs = self.forward_downstream_classification_model(\n",
        "            inputs, labels, scalar_outputs=scalar_outputs\n",
        "        )\n",
        "\n",
        "        return scalar_outputs\n",
        "\n",
        "    def forward_downstream_classification_model(\n",
        "        self, inputs, labels, scalar_outputs=None,\n",
        "    ):\n",
        "        if scalar_outputs is None:\n",
        "            scalar_outputs = {\n",
        "                \"Loss\": torch.zeros(1, device=self.device),\n",
        "            }\n",
        "\n",
        "        z = inputs[\"neutral_sample\"]\n",
        "        z = z.reshape(z.shape[0], -1)\n",
        "        z = self._layer_norm(z)\n",
        "\n",
        "        input_classification_model = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for idx, layer in enumerate(self.model):\n",
        "                z = layer(z)\n",
        "                z = self.act_fn.apply(z)\n",
        "                z = self._layer_norm(z)\n",
        "\n",
        "                if idx >= 1:\n",
        "                    input_classification_model.append(z)\n",
        "\n",
        "        input_classification_model = torch.concat(input_classification_model, dim=-1)\n",
        "\n",
        "        output = self.linear_classifier(input_classification_model.detach())\n",
        "        output = output - torch.max(output, dim=-1, keepdim=True)[0]\n",
        "        classification_loss = self.classification_loss(output, labels[\"class_labels\"])\n",
        "        classification_accuracy = get_accuracy(\n",
        "            output.data, labels[\"class_labels\"]\n",
        "        )\n",
        "\n",
        "        scalar_outputs[\"Loss\"] += classification_loss\n",
        "        scalar_outputs[\"classification_loss\"] = classification_loss\n",
        "        scalar_outputs[\"classification_accuracy\"] = classification_accuracy\n",
        "        return scalar_outputs\n",
        "\n",
        "\n",
        "class ReLU_full_grad(torch.autograd.Function):\n",
        "    \"\"\" ReLU activation function that passes through the gradient irrespective of its input value. \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        return input.clamp(min=0)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return grad_output.clone()\n"
      ],
      "metadata": {
        "id": "jm7Hv_1x1QxV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "utils"
      ],
      "metadata": {
        "id": "fZIVrBBgLvnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_and_optimizer():\n",
        "    model = FF_model()\n",
        "    model = model.to(device)\n",
        "    print(model, \"\\n\")\n",
        "\n",
        "    # Create optimizer with different hyper-parameters for the main model\n",
        "    # and the downstream classification model.\n",
        "    main_model_params = [\n",
        "        p\n",
        "        for p in model.parameters()\n",
        "        if all(p is not x for x in model.classification_loss.parameters())\n",
        "    ]\n",
        "    optimizer = torch.optim.SGD(\n",
        "        [\n",
        "            {\n",
        "                \"params\": main_model_params,\n",
        "                \"lr\": training_learning_rate,\n",
        "                \"weight_decay\": training_weight_decay,\n",
        "                \"momentum\": training_momentum,\n",
        "            },\n",
        "            {\n",
        "                \"params\": model.classification_loss.parameters(),\n",
        "                \"lr\": training_downstream_learning_rate,\n",
        "                \"weight_decay\": training_downstream_weight_decay,\n",
        "                \"momentum\": training_momentum,\n",
        "            },\n",
        "        ]\n",
        "    )\n",
        "    return model, optimizer\n",
        "\n",
        "\n",
        "def get_data(partition):\n",
        "    dataset = FF_MNIST( partition)\n",
        "\n",
        "    # Improve reproducibility in dataloader.\n",
        "    g = torch.Generator()\n",
        "    g.manual_seed(opt_seed)\n",
        "\n",
        "    return torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=input_batch_size,\n",
        "        drop_last=True,\n",
        "        shuffle=True,\n",
        "        worker_init_fn=seed_worker,\n",
        "        generator=g,\n",
        "        num_workers=4,\n",
        "        persistent_workers=True,\n",
        "    )\n",
        "\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2 ** 32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "\n",
        "def get_MNIST_partition(partition):\n",
        "    if partition in [\"train\", \"val\", \"train_val\"]:\n",
        "        mnist = torchvision.datasets.MNIST(\n",
        "            root='./datasets',\n",
        "            train=True,\n",
        "            download=True,\n",
        "            transform=torchvision.transforms.ToTensor(),\n",
        "        )\n",
        "    elif partition in [\"test\"]:\n",
        "        mnist = torchvision.datasets.MNIST(\n",
        "            root='./datasets',\n",
        "            train=False,\n",
        "            download=True,\n",
        "            transform=torchvision.transforms.ToTensor(),\n",
        "        )\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    if partition == \"train\":\n",
        "        mnist = torch.utils.data.Subset(mnist, range(50000))\n",
        "    elif partition == \"val\":\n",
        "        mnist = torchvision.datasets.MNIST(\n",
        "            root='./datasets',\n",
        "            train=True,\n",
        "            download=True,\n",
        "            transform=torchvision.transforms.ToTensor(),\n",
        "        )\n",
        "        mnist = torch.utils.data.Subset(mnist, range(50000, 60000))\n",
        "\n",
        "    return mnist\n",
        "\n",
        "\n",
        "def dict_to_cuda(dict):\n",
        "    for key, value in dict.items():\n",
        "        dict[key] = value.cuda(non_blocking=True)\n",
        "    return dict\n",
        "\n",
        "\n",
        "def preprocess_inputs(inputs, labels):\n",
        "    if \"cuda\" in device:\n",
        "        inputs = dict_to_cuda(inputs)\n",
        "        labels = dict_to_cuda(labels)\n",
        "    return inputs, labels\n",
        "\n",
        "\n",
        "def get_linear_cooldown_lr(epoch, lr):\n",
        "    if epoch > (training_epochs // 2):\n",
        "        return lr * 2 * (1 + training_epochs - epoch) / training_epochs\n",
        "    else:\n",
        "        return lr\n",
        "\n",
        "\n",
        "def update_learning_rate(optimizer, epoch):\n",
        "    optimizer.param_groups[0][\"lr\"] = get_linear_cooldown_lr(\n",
        "         epoch, training_learning_rate\n",
        "    )\n",
        "    optimizer.param_groups[1][\"lr\"] = get_linear_cooldown_lr(\n",
        "         epoch, training_downstream_learning_rate\n",
        "    )\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "def get_accuracy(output, target):\n",
        "    \"\"\"Computes the accuracy.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        prediction = torch.argmax(output, dim=1)\n",
        "        return (prediction == target).sum() / input_batch_size\n",
        "\n",
        "\n",
        "def print_results(partition, iteration_time, scalar_outputs, epoch=None):\n",
        "    if epoch is not None:\n",
        "        print(f\"Epoch {epoch} \\t\", end=\"\")\n",
        "\n",
        "    print(\n",
        "        f\"{partition} \\t \\t\"\n",
        "        f\"Time: {timedelta(seconds=iteration_time)} \\t\",\n",
        "        end=\"\",\n",
        "    )\n",
        "    if scalar_outputs is not None:\n",
        "        for key, value in scalar_outputs.items():\n",
        "            print(f\"{key}: {value:.4f} \\t\", end=\"\")\n",
        "    print()\n",
        "\n",
        "\n",
        "def log_results(result_dict, scalar_outputs, num_steps):\n",
        "    for key, value in scalar_outputs.items():\n",
        "        if isinstance(value, float):\n",
        "            result_dict[key] += value / num_steps\n",
        "        else:\n",
        "            result_dict[key] += value.item() / num_steps\n",
        "    return result_dict\n"
      ],
      "metadata": {
        "id": "XtLAQWaG3ert"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train( model, optimizer):\n",
        "    start_time = time.time()\n",
        "    train_loader = get_data(\"train\")\n",
        "    num_steps_per_epoch = len(train_loader)\n",
        "\n",
        "    for epoch in range(training_epochs):\n",
        "        train_results = defaultdict(float)\n",
        "        optimizer = update_learning_rate(optimizer, epoch)\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = preprocess_inputs(inputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            scalar_outputs = model(inputs, labels)\n",
        "            scalar_outputs[\"Loss\"].backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            train_results = log_results(\n",
        "                train_results, scalar_outputs, num_steps_per_epoch\n",
        "            )\n",
        "\n",
        "        print_results(\"train\", time.time() - start_time, train_results, epoch)\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Validate.\n",
        "        if epoch % training_val_idx == 0 and training_val_idx != -1:\n",
        "            validate_or_test( model, \"val\", epoch=epoch)\n",
        "\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "OoxDqA5cwoOL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def validate_or_test( model, partition, epoch=None):\n",
        "    test_time = time.time()\n",
        "    test_results = defaultdict(float)\n",
        "\n",
        "    data_loader = get_data(partition)\n",
        "    num_steps_per_epoch = len(data_loader)\n",
        "\n",
        "    model.eval()\n",
        "    print(partition)\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = preprocess_inputs(inputs, labels)\n",
        "\n",
        "            scalar_outputs = model.forward_downstream_classification_model(\n",
        "                inputs, labels\n",
        "            )\n",
        "            test_results = log_results(\n",
        "                test_results, scalar_outputs, num_steps_per_epoch\n",
        "            )\n",
        "\n",
        "    print_results(partition, time.time() - test_time, test_results, epoch=epoch)\n",
        "    model.train()\n",
        "\n",
        "\n",
        "def my_main():\n",
        "    model, optimizer = get_model_and_optimizer()\n",
        "    model = train(model, optimizer)\n",
        "    validate_or_test(model, \"val\")\n",
        "\n",
        "    if training_final_test:\n",
        "        validate_or_test( model, \"test\")\n",
        "\n",
        "my_main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VK8ukTNLBkhp",
        "outputId": "b36984aa-3e25-4b6d-b197-b2df35f84dc1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FF_model(\n",
            "  (model): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=1000, bias=True)\n",
            "    (1-2): 2 x Linear(in_features=1000, out_features=1000, bias=True)\n",
            "  )\n",
            "  (ff_loss): BCEWithLogitsLoss()\n",
            "  (linear_classifier): Sequential(\n",
            "    (0): Linear(in_features=2000, out_features=10, bias=False)\n",
            "  )\n",
            "  (classification_loss): CrossEntropyLoss()\n",
            ") \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 \ttrain \t \tTime: 0:00:18.177967 \tLoss: 149.2321 \tPeer Normalization: 0.6602 \tloss_layer_0: 22.2709 \tff_accuracy_layer_0: 0.7688 \tloss_layer_1: 58.9507 \tff_accuracy_layer_1: 0.7426 \tloss_layer_2: 67.4063 \tff_accuracy_layer_2: 0.7088 \tclassification_loss: 0.5844 \tclassification_accuracy: 0.8096 \t\n",
            "Epoch 1 \ttrain \t \tTime: 0:00:18.908163 \tLoss: 33.2938 \tPeer Normalization: 0.5907 \tloss_layer_0: 6.2794 \tff_accuracy_layer_0: 0.8773 \tloss_layer_1: 9.4464 \tff_accuracy_layer_1: 0.8987 \tloss_layer_2: 17.2900 \tff_accuracy_layer_2: 0.8843 \tclassification_loss: 0.2602 \tclassification_accuracy: 0.9205 \t\n",
            "Epoch 2 \ttrain \t \tTime: 0:00:18.021239 \tLoss: 20.6114 \tPeer Normalization: 0.5551 \tloss_layer_0: 4.6982 \tff_accuracy_layer_0: 0.9025 \tloss_layer_1: 6.3514 \tff_accuracy_layer_1: 0.9295 \tloss_layer_2: 9.3454 \tff_accuracy_layer_2: 0.9217 \tclassification_loss: 0.1997 \tclassification_accuracy: 0.9395 \t\n",
            "Epoch 3 \ttrain \t \tTime: 0:00:18.589355 \tLoss: 14.6266 \tPeer Normalization: 0.5485 \tloss_layer_0: 3.5190 \tff_accuracy_layer_0: 0.9212 \tloss_layer_1: 4.2014 \tff_accuracy_layer_1: 0.9483 \tloss_layer_2: 6.7252 \tff_accuracy_layer_2: 0.9369 \tclassification_loss: 0.1646 \tclassification_accuracy: 0.9492 \t\n",
            "Epoch 4 \ttrain \t \tTime: 0:00:19.110480 \tLoss: 13.0215 \tPeer Normalization: 0.5313 \tloss_layer_0: 3.5450 \tff_accuracy_layer_0: 0.9230 \tloss_layer_1: 3.6295 \tff_accuracy_layer_1: 0.9550 \tloss_layer_2: 5.6879 \tff_accuracy_layer_2: 0.9453 \tclassification_loss: 0.1432 \tclassification_accuracy: 0.9551 \t\n",
            "Epoch 5 \ttrain \t \tTime: 0:00:19.350781 \tLoss: 10.9806 \tPeer Normalization: 0.5348 \tloss_layer_0: 2.4840 \tff_accuracy_layer_0: 0.9393 \tloss_layer_1: 2.9354 \tff_accuracy_layer_1: 0.9614 \tloss_layer_2: 5.4218 \tff_accuracy_layer_2: 0.9483 \tclassification_loss: 0.1234 \tclassification_accuracy: 0.9617 \t\n",
            "Epoch 6 \ttrain \t \tTime: 0:00:19.387229 \tLoss: 10.0104 \tPeer Normalization: 0.5209 \tloss_layer_0: 2.7964 \tff_accuracy_layer_0: 0.9369 \tloss_layer_1: 2.5395 \tff_accuracy_layer_1: 0.9648 \tloss_layer_2: 4.5441 \tff_accuracy_layer_2: 0.9530 \tclassification_loss: 0.1148 \tclassification_accuracy: 0.9650 \t\n",
            "Epoch 7 \ttrain \t \tTime: 0:00:17.955132 \tLoss: 9.6263 \tPeer Normalization: 0.5205 \tloss_layer_0: 2.7874 \tff_accuracy_layer_0: 0.9399 \tloss_layer_1: 2.2031 \tff_accuracy_layer_1: 0.9686 \tloss_layer_2: 4.5137 \tff_accuracy_layer_2: 0.9553 \tclassification_loss: 0.1065 \tclassification_accuracy: 0.9678 \t\n",
            "Epoch 8 \ttrain \t \tTime: 0:00:19.836739 \tLoss: 8.4905 \tPeer Normalization: 0.5211 \tloss_layer_0: 2.2134 \tff_accuracy_layer_0: 0.9485 \tloss_layer_1: 2.0970 \tff_accuracy_layer_1: 0.9696 \tloss_layer_2: 4.0711 \tff_accuracy_layer_2: 0.9578 \tclassification_loss: 0.0934 \tclassification_accuracy: 0.9712 \t\n",
            "Epoch 9 \ttrain \t \tTime: 0:00:18.702212 \tLoss: 7.5430 \tPeer Normalization: 0.5090 \tloss_layer_0: 2.0537 \tff_accuracy_layer_0: 0.9523 \tloss_layer_1: 1.7859 \tff_accuracy_layer_1: 0.9734 \tloss_layer_2: 3.6016 \tff_accuracy_layer_2: 0.9614 \tclassification_loss: 0.0865 \tclassification_accuracy: 0.9728 \t\n",
            "Epoch 10 \ttrain \t \tTime: 0:00:23.422616 \tLoss: 6.7933 \tPeer Normalization: 0.5180 \tloss_layer_0: 1.5094 \tff_accuracy_layer_0: 0.9597 \tloss_layer_1: 1.6831 \tff_accuracy_layer_1: 0.9741 \tloss_layer_2: 3.5064 \tff_accuracy_layer_2: 0.9620 \tclassification_loss: 0.0788 \tclassification_accuracy: 0.9758 \t\n",
            "Epoch 11 \ttrain \t \tTime: 0:00:23.357143 \tLoss: 7.2976 \tPeer Normalization: 0.4918 \tloss_layer_0: 2.0123 \tff_accuracy_layer_0: 0.9515 \tloss_layer_1: 1.6503 \tff_accuracy_layer_1: 0.9753 \tloss_layer_2: 3.5433 \tff_accuracy_layer_2: 0.9624 \tclassification_loss: 0.0769 \tclassification_accuracy: 0.9767 \t\n",
            "Epoch 12 \ttrain \t \tTime: 0:00:18.128039 \tLoss: 7.0879 \tPeer Normalization: 0.4766 \tloss_layer_0: 2.1792 \tff_accuracy_layer_0: 0.9522 \tloss_layer_1: 1.4948 \tff_accuracy_layer_1: 0.9769 \tloss_layer_2: 3.3258 \tff_accuracy_layer_2: 0.9634 \tclassification_loss: 0.0738 \tclassification_accuracy: 0.9777 \t\n",
            "Epoch 13 \ttrain \t \tTime: 0:00:19.200094 \tLoss: 5.8927 \tPeer Normalization: 0.4987 \tloss_layer_0: 1.4707 \tff_accuracy_layer_0: 0.9615 \tloss_layer_1: 1.3165 \tff_accuracy_layer_1: 0.9792 \tloss_layer_2: 3.0243 \tff_accuracy_layer_2: 0.9664 \tclassification_loss: 0.0662 \tclassification_accuracy: 0.9797 \t\n",
            "Epoch 14 \ttrain \t \tTime: 0:00:18.179027 \tLoss: 5.2385 \tPeer Normalization: 0.5002 \tloss_layer_0: 1.4438 \tff_accuracy_layer_0: 0.9628 \tloss_layer_1: 1.1510 \tff_accuracy_layer_1: 0.9814 \tloss_layer_2: 2.5653 \tff_accuracy_layer_2: 0.9695 \tclassification_loss: 0.0634 \tclassification_accuracy: 0.9806 \t\n",
            "Epoch 15 \ttrain \t \tTime: 0:00:18.014054 \tLoss: 5.3691 \tPeer Normalization: 0.4910 \tloss_layer_0: 1.5170 \tff_accuracy_layer_0: 0.9614 \tloss_layer_1: 1.2011 \tff_accuracy_layer_1: 0.9809 \tloss_layer_2: 2.5766 \tff_accuracy_layer_2: 0.9690 \tclassification_loss: 0.0596 \tclassification_accuracy: 0.9813 \t\n",
            "Epoch 16 \ttrain \t \tTime: 0:00:19.097898 \tLoss: 5.6988 \tPeer Normalization: 0.4761 \tloss_layer_0: 1.3781 \tff_accuracy_layer_0: 0.9639 \tloss_layer_1: 1.2723 \tff_accuracy_layer_1: 0.9804 \tloss_layer_2: 2.9752 \tff_accuracy_layer_2: 0.9665 \tclassification_loss: 0.0589 \tclassification_accuracy: 0.9818 \t\n",
            "Epoch 17 \ttrain \t \tTime: 0:00:18.075155 \tLoss: 4.4895 \tPeer Normalization: 0.4787 \tloss_layer_0: 1.0407 \tff_accuracy_layer_0: 0.9704 \tloss_layer_1: 0.9864 \tff_accuracy_layer_1: 0.9832 \tloss_layer_2: 2.3956 \tff_accuracy_layer_2: 0.9706 \tclassification_loss: 0.0525 \tclassification_accuracy: 0.9843 \t\n",
            "Epoch 18 \ttrain \t \tTime: 0:00:20.555743 \tLoss: 4.7428 \tPeer Normalization: 0.4842 \tloss_layer_0: 1.1192 \tff_accuracy_layer_0: 0.9700 \tloss_layer_1: 1.0568 \tff_accuracy_layer_1: 0.9826 \tloss_layer_2: 2.5004 \tff_accuracy_layer_2: 0.9696 \tclassification_loss: 0.0518 \tclassification_accuracy: 0.9837 \t\n",
            "Epoch 19 \ttrain \t \tTime: 0:00:18.441691 \tLoss: 4.6578 \tPeer Normalization: 0.4776 \tloss_layer_0: 1.2167 \tff_accuracy_layer_0: 0.9671 \tloss_layer_1: 0.9595 \tff_accuracy_layer_1: 0.9839 \tloss_layer_2: 2.4170 \tff_accuracy_layer_2: 0.9704 \tclassification_loss: 0.0503 \tclassification_accuracy: 0.9847 \t\n",
            "Epoch 20 \ttrain \t \tTime: 0:00:19.012800 \tLoss: 4.0042 \tPeer Normalization: 0.4898 \tloss_layer_0: 0.9368 \tff_accuracy_layer_0: 0.9736 \tloss_layer_1: 0.8597 \tff_accuracy_layer_1: 0.9858 \tloss_layer_2: 2.1457 \tff_accuracy_layer_2: 0.9732 \tclassification_loss: 0.0474 \tclassification_accuracy: 0.9853 \t\n",
            "Epoch 21 \ttrain \t \tTime: 0:00:18.100984 \tLoss: 4.0959 \tPeer Normalization: 0.4858 \tloss_layer_0: 1.1887 \tff_accuracy_layer_0: 0.9672 \tloss_layer_1: 0.8238 \tff_accuracy_layer_1: 0.9858 \tloss_layer_2: 2.0223 \tff_accuracy_layer_2: 0.9737 \tclassification_loss: 0.0465 \tclassification_accuracy: 0.9854 \t\n",
            "Epoch 22 \ttrain \t \tTime: 0:00:18.775775 \tLoss: 3.7287 \tPeer Normalization: 0.4866 \tloss_layer_0: 0.9050 \tff_accuracy_layer_0: 0.9744 \tloss_layer_1: 0.7964 \tff_accuracy_layer_1: 0.9862 \tloss_layer_2: 1.9700 \tff_accuracy_layer_2: 0.9737 \tclassification_loss: 0.0427 \tclassification_accuracy: 0.9865 \t\n",
            "Epoch 23 \ttrain \t \tTime: 0:00:18.216883 \tLoss: 3.8145 \tPeer Normalization: 0.4947 \tloss_layer_0: 0.9556 \tff_accuracy_layer_0: 0.9731 \tloss_layer_1: 0.7532 \tff_accuracy_layer_1: 0.9868 \tloss_layer_2: 2.0482 \tff_accuracy_layer_2: 0.9737 \tclassification_loss: 0.0426 \tclassification_accuracy: 0.9870 \t\n",
            "Epoch 24 \ttrain \t \tTime: 0:00:18.173864 \tLoss: 3.5779 \tPeer Normalization: 0.4907 \tloss_layer_0: 0.8435 \tff_accuracy_layer_0: 0.9759 \tloss_layer_1: 0.7523 \tff_accuracy_layer_1: 0.9866 \tloss_layer_2: 1.9276 \tff_accuracy_layer_2: 0.9737 \tclassification_loss: 0.0399 \tclassification_accuracy: 0.9875 \t\n",
            "Epoch 25 \ttrain \t \tTime: 0:00:20.489244 \tLoss: 3.5037 \tPeer Normalization: 0.4846 \tloss_layer_0: 0.9068 \tff_accuracy_layer_0: 0.9731 \tloss_layer_1: 0.7289 \tff_accuracy_layer_1: 0.9865 \tloss_layer_2: 1.8140 \tff_accuracy_layer_2: 0.9755 \tclassification_loss: 0.0395 \tclassification_accuracy: 0.9876 \t\n",
            "Epoch 26 \ttrain \t \tTime: 0:00:18.176649 \tLoss: 3.4856 \tPeer Normalization: 0.4699 \tloss_layer_0: 1.0046 \tff_accuracy_layer_0: 0.9719 \tloss_layer_1: 0.6782 \tff_accuracy_layer_1: 0.9880 \tloss_layer_2: 1.7502 \tff_accuracy_layer_2: 0.9758 \tclassification_loss: 0.0386 \tclassification_accuracy: 0.9881 \t\n",
            "Epoch 27 \ttrain \t \tTime: 0:00:19.200996 \tLoss: 3.2422 \tPeer Normalization: 0.4823 \tloss_layer_0: 0.8665 \tff_accuracy_layer_0: 0.9750 \tloss_layer_1: 0.6002 \tff_accuracy_layer_1: 0.9891 \tloss_layer_2: 1.7232 \tff_accuracy_layer_2: 0.9767 \tclassification_loss: 0.0379 \tclassification_accuracy: 0.9886 \t\n",
            "Epoch 28 \ttrain \t \tTime: 0:00:18.084535 \tLoss: 3.2463 \tPeer Normalization: 0.4687 \tloss_layer_0: 0.8761 \tff_accuracy_layer_0: 0.9744 \tloss_layer_1: 0.6176 \tff_accuracy_layer_1: 0.9888 \tloss_layer_2: 1.7036 \tff_accuracy_layer_2: 0.9757 \tclassification_loss: 0.0349 \tclassification_accuracy: 0.9894 \t\n",
            "Epoch 29 \ttrain \t \tTime: 0:00:19.447274 \tLoss: 3.1365 \tPeer Normalization: 0.4702 \tloss_layer_0: 0.7706 \tff_accuracy_layer_0: 0.9773 \tloss_layer_1: 0.5984 \tff_accuracy_layer_1: 0.9895 \tloss_layer_2: 1.7190 \tff_accuracy_layer_2: 0.9761 \tclassification_loss: 0.0345 \tclassification_accuracy: 0.9890 \t\n",
            "Epoch 30 \ttrain \t \tTime: 0:00:18.139750 \tLoss: 2.7717 \tPeer Normalization: 0.4709 \tloss_layer_0: 0.7073 \tff_accuracy_layer_0: 0.9780 \tloss_layer_1: 0.5029 \tff_accuracy_layer_1: 0.9903 \tloss_layer_2: 1.5162 \tff_accuracy_layer_2: 0.9780 \tclassification_loss: 0.0311 \tclassification_accuracy: 0.9905 \t\n",
            "Epoch 31 \ttrain \t \tTime: 0:00:18.554758 \tLoss: 2.8951 \tPeer Normalization: 0.4680 \tloss_layer_0: 0.7512 \tff_accuracy_layer_0: 0.9777 \tloss_layer_1: 0.5498 \tff_accuracy_layer_1: 0.9896 \tloss_layer_2: 1.5493 \tff_accuracy_layer_2: 0.9777 \tclassification_loss: 0.0308 \tclassification_accuracy: 0.9906 \t\n",
            "Epoch 32 \ttrain \t \tTime: 0:00:20.143483 \tLoss: 2.6859 \tPeer Normalization: 0.4790 \tloss_layer_0: 0.6827 \tff_accuracy_layer_0: 0.9787 \tloss_layer_1: 0.5116 \tff_accuracy_layer_1: 0.9904 \tloss_layer_2: 1.4465 \tff_accuracy_layer_2: 0.9781 \tclassification_loss: 0.0308 \tclassification_accuracy: 0.9907 \t\n",
            "Epoch 33 \ttrain \t \tTime: 0:00:18.012397 \tLoss: 2.9246 \tPeer Normalization: 0.4688 \tloss_layer_0: 0.8167 \tff_accuracy_layer_0: 0.9771 \tloss_layer_1: 0.5225 \tff_accuracy_layer_1: 0.9901 \tloss_layer_2: 1.5411 \tff_accuracy_layer_2: 0.9775 \tclassification_loss: 0.0302 \tclassification_accuracy: 0.9906 \t\n",
            "Epoch 34 \ttrain \t \tTime: 0:00:19.074099 \tLoss: 2.8210 \tPeer Normalization: 0.4634 \tloss_layer_0: 0.7542 \tff_accuracy_layer_0: 0.9779 \tloss_layer_1: 0.4954 \tff_accuracy_layer_1: 0.9904 \tloss_layer_2: 1.5288 \tff_accuracy_layer_2: 0.9777 \tclassification_loss: 0.0287 \tclassification_accuracy: 0.9909 \t\n",
            "Epoch 35 \ttrain \t \tTime: 0:00:17.833294 \tLoss: 2.7172 \tPeer Normalization: 0.4701 \tloss_layer_0: 0.6807 \tff_accuracy_layer_0: 0.9794 \tloss_layer_1: 0.5125 \tff_accuracy_layer_1: 0.9900 \tloss_layer_2: 1.4813 \tff_accuracy_layer_2: 0.9778 \tclassification_loss: 0.0286 \tclassification_accuracy: 0.9914 \t\n",
            "Epoch 36 \ttrain \t \tTime: 0:00:18.915538 \tLoss: 2.6177 \tPeer Normalization: 0.4732 \tloss_layer_0: 0.6632 \tff_accuracy_layer_0: 0.9798 \tloss_layer_1: 0.4520 \tff_accuracy_layer_1: 0.9909 \tloss_layer_2: 1.4615 \tff_accuracy_layer_2: 0.9781 \tclassification_loss: 0.0268 \tclassification_accuracy: 0.9915 \t\n",
            "Epoch 37 \ttrain \t \tTime: 0:00:18.591458 \tLoss: 2.3852 \tPeer Normalization: 0.4705 \tloss_layer_0: 0.5479 \tff_accuracy_layer_0: 0.9828 \tloss_layer_1: 0.4616 \tff_accuracy_layer_1: 0.9907 \tloss_layer_2: 1.3340 \tff_accuracy_layer_2: 0.9799 \tclassification_loss: 0.0275 \tclassification_accuracy: 0.9918 \t\n",
            "Epoch 38 \ttrain \t \tTime: 0:00:19.470205 \tLoss: 2.2982 \tPeer Normalization: 0.4787 \tloss_layer_0: 0.5422 \tff_accuracy_layer_0: 0.9826 \tloss_layer_1: 0.4401 \tff_accuracy_layer_1: 0.9911 \tloss_layer_2: 1.2774 \tff_accuracy_layer_2: 0.9806 \tclassification_loss: 0.0241 \tclassification_accuracy: 0.9926 \t\n",
            "Epoch 39 \ttrain \t \tTime: 0:00:20.104710 \tLoss: 2.5045 \tPeer Normalization: 0.4825 \tloss_layer_0: 0.6259 \tff_accuracy_layer_0: 0.9810 \tloss_layer_1: 0.4483 \tff_accuracy_layer_1: 0.9916 \tloss_layer_2: 1.3914 \tff_accuracy_layer_2: 0.9794 \tclassification_loss: 0.0246 \tclassification_accuracy: 0.9926 \t\n",
            "Epoch 40 \ttrain \t \tTime: 0:00:18.455520 \tLoss: 2.5426 \tPeer Normalization: 0.4631 \tloss_layer_0: 0.8171 \tff_accuracy_layer_0: 0.9763 \tloss_layer_1: 0.4034 \tff_accuracy_layer_1: 0.9921 \tloss_layer_2: 1.2830 \tff_accuracy_layer_2: 0.9796 \tclassification_loss: 0.0252 \tclassification_accuracy: 0.9923 \t\n",
            "Epoch 41 \ttrain \t \tTime: 0:00:18.978584 \tLoss: 2.4242 \tPeer Normalization: 0.4615 \tloss_layer_0: 0.5917 \tff_accuracy_layer_0: 0.9811 \tloss_layer_1: 0.4317 \tff_accuracy_layer_1: 0.9920 \tloss_layer_2: 1.3630 \tff_accuracy_layer_2: 0.9798 \tclassification_loss: 0.0239 \tclassification_accuracy: 0.9927 \t\n",
            "Epoch 42 \ttrain \t \tTime: 0:00:18.027924 \tLoss: 2.1689 \tPeer Normalization: 0.4643 \tloss_layer_0: 0.5738 \tff_accuracy_layer_0: 0.9817 \tloss_layer_1: 0.3625 \tff_accuracy_layer_1: 0.9925 \tloss_layer_2: 1.1968 \tff_accuracy_layer_2: 0.9811 \tclassification_loss: 0.0218 \tclassification_accuracy: 0.9930 \t\n",
            "Epoch 43 \ttrain \t \tTime: 0:00:19.170779 \tLoss: 2.0956 \tPeer Normalization: 0.4727 \tloss_layer_0: 0.5528 \tff_accuracy_layer_0: 0.9826 \tloss_layer_1: 0.3888 \tff_accuracy_layer_1: 0.9925 \tloss_layer_2: 1.1184 \tff_accuracy_layer_2: 0.9824 \tclassification_loss: 0.0213 \tclassification_accuracy: 0.9937 \t\n",
            "Epoch 44 \ttrain \t \tTime: 0:00:17.976694 \tLoss: 2.1503 \tPeer Normalization: 0.4758 \tloss_layer_0: 0.5661 \tff_accuracy_layer_0: 0.9824 \tloss_layer_1: 0.3989 \tff_accuracy_layer_1: 0.9920 \tloss_layer_2: 1.1491 \tff_accuracy_layer_2: 0.9826 \tclassification_loss: 0.0220 \tclassification_accuracy: 0.9932 \t\n",
            "Epoch 45 \ttrain \t \tTime: 0:00:19.048743 \tLoss: 2.1810 \tPeer Normalization: 0.4795 \tloss_layer_0: 0.6128 \tff_accuracy_layer_0: 0.9812 \tloss_layer_1: 0.3717 \tff_accuracy_layer_1: 0.9927 \tloss_layer_2: 1.1601 \tff_accuracy_layer_2: 0.9818 \tclassification_loss: 0.0220 \tclassification_accuracy: 0.9933 \t\n",
            "Epoch 46 \ttrain \t \tTime: 0:00:19.706033 \tLoss: 2.0620 \tPeer Normalization: 0.4712 \tloss_layer_0: 0.5010 \tff_accuracy_layer_0: 0.9838 \tloss_layer_1: 0.4070 \tff_accuracy_layer_1: 0.9917 \tloss_layer_2: 1.1198 \tff_accuracy_layer_2: 0.9821 \tclassification_loss: 0.0200 \tclassification_accuracy: 0.9937 \t\n",
            "Epoch 47 \ttrain \t \tTime: 0:00:18.797477 \tLoss: 1.9585 \tPeer Normalization: 0.4608 \tloss_layer_0: 0.5011 \tff_accuracy_layer_0: 0.9839 \tloss_layer_1: 0.3276 \tff_accuracy_layer_1: 0.9932 \tloss_layer_2: 1.0966 \tff_accuracy_layer_2: 0.9827 \tclassification_loss: 0.0194 \tclassification_accuracy: 0.9942 \t\n",
            "Epoch 48 \ttrain \t \tTime: 0:00:19.044581 \tLoss: 1.9894 \tPeer Normalization: 0.4734 \tloss_layer_0: 0.4951 \tff_accuracy_layer_0: 0.9838 \tloss_layer_1: 0.3037 \tff_accuracy_layer_1: 0.9936 \tloss_layer_2: 1.1578 \tff_accuracy_layer_2: 0.9818 \tclassification_loss: 0.0187 \tclassification_accuracy: 0.9937 \t\n",
            "Epoch 49 \ttrain \t \tTime: 0:00:17.905559 \tLoss: 2.2081 \tPeer Normalization: 0.4546 \tloss_layer_0: 0.6135 \tff_accuracy_layer_0: 0.9812 \tloss_layer_1: 0.3618 \tff_accuracy_layer_1: 0.9926 \tloss_layer_2: 1.2001 \tff_accuracy_layer_2: 0.9818 \tclassification_loss: 0.0191 \tclassification_accuracy: 0.9942 \t\n",
            "Epoch 50 \ttrain \t \tTime: 0:00:19.520899 \tLoss: 2.1719 \tPeer Normalization: 0.4523 \tloss_layer_0: 0.5498 \tff_accuracy_layer_0: 0.9822 \tloss_layer_1: 0.3228 \tff_accuracy_layer_1: 0.9937 \tloss_layer_2: 1.2677 \tff_accuracy_layer_2: 0.9811 \tclassification_loss: 0.0181 \tclassification_accuracy: 0.9943 \t\n",
            "Epoch 51 \ttrain \t \tTime: 0:00:18.116130 \tLoss: 1.8089 \tPeer Normalization: 0.4542 \tloss_layer_0: 0.4345 \tff_accuracy_layer_0: 0.9851 \tloss_layer_1: 0.2900 \tff_accuracy_layer_1: 0.9938 \tloss_layer_2: 1.0534 \tff_accuracy_layer_2: 0.9829 \tclassification_loss: 0.0173 \tclassification_accuracy: 0.9948 \t\n",
            "Epoch 52 \ttrain \t \tTime: 0:00:19.004874 \tLoss: 1.7893 \tPeer Normalization: 0.4610 \tloss_layer_0: 0.4147 \tff_accuracy_layer_0: 0.9857 \tloss_layer_1: 0.3184 \tff_accuracy_layer_1: 0.9937 \tloss_layer_2: 1.0257 \tff_accuracy_layer_2: 0.9836 \tclassification_loss: 0.0166 \tclassification_accuracy: 0.9948 \t\n",
            "Epoch 53 \ttrain \t \tTime: 0:00:19.375754 \tLoss: 1.6052 \tPeer Normalization: 0.4654 \tloss_layer_0: 0.3931 \tff_accuracy_layer_0: 0.9863 \tloss_layer_1: 0.2788 \tff_accuracy_layer_1: 0.9944 \tloss_layer_2: 0.9029 \tff_accuracy_layer_2: 0.9849 \tclassification_loss: 0.0164 \tclassification_accuracy: 0.9953 \t\n",
            "Epoch 54 \ttrain \t \tTime: 0:00:19.228376 \tLoss: 1.7670 \tPeer Normalization: 0.4637 \tloss_layer_0: 0.4332 \tff_accuracy_layer_0: 0.9854 \tloss_layer_1: 0.2764 \tff_accuracy_layer_1: 0.9941 \tloss_layer_2: 1.0282 \tff_accuracy_layer_2: 0.9834 \tclassification_loss: 0.0153 \tclassification_accuracy: 0.9953 \t\n",
            "Epoch 55 \ttrain \t \tTime: 0:00:18.115443 \tLoss: 1.4921 \tPeer Normalization: 0.4681 \tloss_layer_0: 0.3851 \tff_accuracy_layer_0: 0.9865 \tloss_layer_1: 0.2403 \tff_accuracy_layer_1: 0.9946 \tloss_layer_2: 0.8377 \tff_accuracy_layer_2: 0.9854 \tclassification_loss: 0.0149 \tclassification_accuracy: 0.9955 \t\n",
            "Epoch 56 \ttrain \t \tTime: 0:00:19.255208 \tLoss: 1.2911 \tPeer Normalization: 0.4704 \tloss_layer_0: 0.3187 \tff_accuracy_layer_0: 0.9884 \tloss_layer_1: 0.2102 \tff_accuracy_layer_1: 0.9950 \tloss_layer_2: 0.7333 \tff_accuracy_layer_2: 0.9864 \tclassification_loss: 0.0148 \tclassification_accuracy: 0.9958 \t\n",
            "Epoch 57 \ttrain \t \tTime: 0:00:18.186487 \tLoss: 1.2459 \tPeer Normalization: 0.4691 \tloss_layer_0: 0.3241 \tff_accuracy_layer_0: 0.9883 \tloss_layer_1: 0.1905 \tff_accuracy_layer_1: 0.9954 \tloss_layer_2: 0.7033 \tff_accuracy_layer_2: 0.9865 \tclassification_loss: 0.0140 \tclassification_accuracy: 0.9961 \t\n",
            "Epoch 58 \ttrain \t \tTime: 0:00:18.275550 \tLoss: 1.2563 \tPeer Normalization: 0.4789 \tloss_layer_0: 0.3244 \tff_accuracy_layer_0: 0.9883 \tloss_layer_1: 0.2029 \tff_accuracy_layer_1: 0.9952 \tloss_layer_2: 0.7013 \tff_accuracy_layer_2: 0.9865 \tclassification_loss: 0.0134 \tclassification_accuracy: 0.9963 \t\n",
            "Epoch 59 \ttrain \t \tTime: 0:00:18.939461 \tLoss: 1.1895 \tPeer Normalization: 0.4813 \tloss_layer_0: 0.3400 \tff_accuracy_layer_0: 0.9878 \tloss_layer_1: 0.1685 \tff_accuracy_layer_1: 0.9958 \tloss_layer_2: 0.6532 \tff_accuracy_layer_2: 0.9870 \tclassification_loss: 0.0133 \tclassification_accuracy: 0.9962 \t\n",
            "Epoch 60 \ttrain \t \tTime: 0:00:19.631460 \tLoss: 1.0358 \tPeer Normalization: 0.4818 \tloss_layer_0: 0.2260 \tff_accuracy_layer_0: 0.9908 \tloss_layer_1: 0.1717 \tff_accuracy_layer_1: 0.9960 \tloss_layer_2: 0.6111 \tff_accuracy_layer_2: 0.9884 \tclassification_loss: 0.0126 \tclassification_accuracy: 0.9966 \t\n",
            "Epoch 61 \ttrain \t \tTime: 0:00:19.752647 \tLoss: 1.0321 \tPeer Normalization: 0.4812 \tloss_layer_0: 0.2700 \tff_accuracy_layer_0: 0.9896 \tloss_layer_1: 0.1507 \tff_accuracy_layer_1: 0.9962 \tloss_layer_2: 0.5851 \tff_accuracy_layer_2: 0.9883 \tclassification_loss: 0.0119 \tclassification_accuracy: 0.9966 \t\n",
            "Epoch 62 \ttrain \t \tTime: 0:00:18.095092 \tLoss: 0.9521 \tPeer Normalization: 0.4826 \tloss_layer_0: 0.2101 \tff_accuracy_layer_0: 0.9912 \tloss_layer_1: 0.1506 \tff_accuracy_layer_1: 0.9963 \tloss_layer_2: 0.5657 \tff_accuracy_layer_2: 0.9882 \tclassification_loss: 0.0112 \tclassification_accuracy: 0.9971 \t\n",
            "Epoch 63 \ttrain \t \tTime: 0:00:19.301534 \tLoss: 0.9316 \tPeer Normalization: 0.4840 \tloss_layer_0: 0.2308 \tff_accuracy_layer_0: 0.9906 \tloss_layer_1: 0.1415 \tff_accuracy_layer_1: 0.9962 \tloss_layer_2: 0.5335 \tff_accuracy_layer_2: 0.9891 \tclassification_loss: 0.0113 \tclassification_accuracy: 0.9970 \t\n",
            "Epoch 64 \ttrain \t \tTime: 0:00:18.086036 \tLoss: 0.8922 \tPeer Normalization: 0.4796 \tloss_layer_0: 0.2641 \tff_accuracy_layer_0: 0.9896 \tloss_layer_1: 0.1330 \tff_accuracy_layer_1: 0.9965 \tloss_layer_2: 0.4697 \tff_accuracy_layer_2: 0.9891 \tclassification_loss: 0.0110 \tclassification_accuracy: 0.9972 \t\n",
            "Epoch 65 \ttrain \t \tTime: 0:00:19.031777 \tLoss: 0.7667 \tPeer Normalization: 0.4800 \tloss_layer_0: 0.1901 \tff_accuracy_layer_0: 0.9921 \tloss_layer_1: 0.1095 \tff_accuracy_layer_1: 0.9972 \tloss_layer_2: 0.4431 \tff_accuracy_layer_2: 0.9904 \tclassification_loss: 0.0096 \tclassification_accuracy: 0.9976 \t\n",
            "Epoch 66 \ttrain \t \tTime: 0:00:19.336888 \tLoss: 0.7538 \tPeer Normalization: 0.4844 \tloss_layer_0: 0.1927 \tff_accuracy_layer_0: 0.9920 \tloss_layer_1: 0.1055 \tff_accuracy_layer_1: 0.9973 \tloss_layer_2: 0.4310 \tff_accuracy_layer_2: 0.9904 \tclassification_loss: 0.0100 \tclassification_accuracy: 0.9975 \t\n",
            "Epoch 67 \ttrain \t \tTime: 0:00:19.129621 \tLoss: 0.6576 \tPeer Normalization: 0.4857 \tloss_layer_0: 0.1600 \tff_accuracy_layer_0: 0.9926 \tloss_layer_1: 0.1047 \tff_accuracy_layer_1: 0.9971 \tloss_layer_2: 0.3687 \tff_accuracy_layer_2: 0.9915 \tclassification_loss: 0.0096 \tclassification_accuracy: 0.9974 \t\n",
            "Epoch 68 \ttrain \t \tTime: 0:00:18.095860 \tLoss: 0.6920 \tPeer Normalization: 0.4924 \tloss_layer_0: 0.1734 \tff_accuracy_layer_0: 0.9932 \tloss_layer_1: 0.0984 \tff_accuracy_layer_1: 0.9974 \tloss_layer_2: 0.3962 \tff_accuracy_layer_2: 0.9905 \tclassification_loss: 0.0092 \tclassification_accuracy: 0.9977 \t\n",
            "Epoch 69 \ttrain \t \tTime: 0:00:19.423236 \tLoss: 0.6400 \tPeer Normalization: 0.4927 \tloss_layer_0: 0.1471 \tff_accuracy_layer_0: 0.9934 \tloss_layer_1: 0.0934 \tff_accuracy_layer_1: 0.9972 \tloss_layer_2: 0.3756 \tff_accuracy_layer_2: 0.9916 \tclassification_loss: 0.0090 \tclassification_accuracy: 0.9978 \t\n",
            "Epoch 70 \ttrain \t \tTime: 0:00:18.121116 \tLoss: 0.5739 \tPeer Normalization: 0.4934 \tloss_layer_0: 0.1473 \tff_accuracy_layer_0: 0.9933 \tloss_layer_1: 0.0824 \tff_accuracy_layer_1: 0.9975 \tloss_layer_2: 0.3205 \tff_accuracy_layer_2: 0.9920 \tclassification_loss: 0.0088 \tclassification_accuracy: 0.9979 \t\n",
            "Epoch 71 \ttrain \t \tTime: 0:00:18.865992 \tLoss: 0.5112 \tPeer Normalization: 0.4964 \tloss_layer_0: 0.1264 \tff_accuracy_layer_0: 0.9940 \tloss_layer_1: 0.0686 \tff_accuracy_layer_1: 0.9977 \tloss_layer_2: 0.2927 \tff_accuracy_layer_2: 0.9921 \tclassification_loss: 0.0086 \tclassification_accuracy: 0.9981 \t\n",
            "Epoch 72 \ttrain \t \tTime: 0:00:18.602375 \tLoss: 0.5121 \tPeer Normalization: 0.5078 \tloss_layer_0: 0.1224 \tff_accuracy_layer_0: 0.9939 \tloss_layer_1: 0.0802 \tff_accuracy_layer_1: 0.9977 \tloss_layer_2: 0.2857 \tff_accuracy_layer_2: 0.9931 \tclassification_loss: 0.0086 \tclassification_accuracy: 0.9980 \t\n",
            "Epoch 73 \ttrain \t \tTime: 0:00:19.224859 \tLoss: 0.4458 \tPeer Normalization: 0.5048 \tloss_layer_0: 0.1116 \tff_accuracy_layer_0: 0.9941 \tloss_layer_1: 0.0516 \tff_accuracy_layer_1: 0.9981 \tloss_layer_2: 0.2593 \tff_accuracy_layer_2: 0.9930 \tclassification_loss: 0.0082 \tclassification_accuracy: 0.9982 \t\n",
            "Epoch 74 \ttrain \t \tTime: 0:00:18.709539 \tLoss: 0.4239 \tPeer Normalization: 0.5089 \tloss_layer_0: 0.1095 \tff_accuracy_layer_0: 0.9946 \tloss_layer_1: 0.0495 \tff_accuracy_layer_1: 0.9982 \tloss_layer_2: 0.2419 \tff_accuracy_layer_2: 0.9934 \tclassification_loss: 0.0078 \tclassification_accuracy: 0.9986 \t\n",
            "Epoch 75 \ttrain \t \tTime: 0:00:18.095232 \tLoss: 0.4233 \tPeer Normalization: 0.5094 \tloss_layer_0: 0.1028 \tff_accuracy_layer_0: 0.9949 \tloss_layer_1: 0.0652 \tff_accuracy_layer_1: 0.9981 \tloss_layer_2: 0.2319 \tff_accuracy_layer_2: 0.9933 \tclassification_loss: 0.0081 \tclassification_accuracy: 0.9981 \t\n",
            "Epoch 76 \ttrain \t \tTime: 0:00:19.240983 \tLoss: 0.3770 \tPeer Normalization: 0.5141 \tloss_layer_0: 0.0999 \tff_accuracy_layer_0: 0.9949 \tloss_layer_1: 0.0455 \tff_accuracy_layer_1: 0.9983 \tloss_layer_2: 0.2086 \tff_accuracy_layer_2: 0.9940 \tclassification_loss: 0.0076 \tclassification_accuracy: 0.9984 \t\n",
            "Epoch 77 \ttrain \t \tTime: 0:00:18.070220 \tLoss: 0.3577 \tPeer Normalization: 0.5172 \tloss_layer_0: 0.0894 \tff_accuracy_layer_0: 0.9954 \tloss_layer_1: 0.0448 \tff_accuracy_layer_1: 0.9983 \tloss_layer_2: 0.2005 \tff_accuracy_layer_2: 0.9941 \tclassification_loss: 0.0075 \tclassification_accuracy: 0.9983 \t\n",
            "Epoch 78 \ttrain \t \tTime: 0:00:18.859336 \tLoss: 0.3217 \tPeer Normalization: 0.5267 \tloss_layer_0: 0.0686 \tff_accuracy_layer_0: 0.9962 \tloss_layer_1: 0.0413 \tff_accuracy_layer_1: 0.9985 \tloss_layer_2: 0.1890 \tff_accuracy_layer_2: 0.9942 \tclassification_loss: 0.0070 \tclassification_accuracy: 0.9987 \t\n",
            "Epoch 79 \ttrain \t \tTime: 0:00:17.967251 \tLoss: 0.2983 \tPeer Normalization: 0.5191 \tloss_layer_0: 0.0767 \tff_accuracy_layer_0: 0.9959 \tloss_layer_1: 0.0383 \tff_accuracy_layer_1: 0.9987 \tloss_layer_2: 0.1610 \tff_accuracy_layer_2: 0.9947 \tclassification_loss: 0.0068 \tclassification_accuracy: 0.9988 \t\n",
            "Epoch 80 \ttrain \t \tTime: 0:00:19.985657 \tLoss: 0.2996 \tPeer Normalization: 0.5193 \tloss_layer_0: 0.0737 \tff_accuracy_layer_0: 0.9961 \tloss_layer_1: 0.0378 \tff_accuracy_layer_1: 0.9986 \tloss_layer_2: 0.1657 \tff_accuracy_layer_2: 0.9946 \tclassification_loss: 0.0068 \tclassification_accuracy: 0.9986 \t\n",
            "Epoch 81 \ttrain \t \tTime: 0:00:18.492863 \tLoss: 0.2223 \tPeer Normalization: 0.5201 \tloss_layer_0: 0.0459 \tff_accuracy_layer_0: 0.9973 \tloss_layer_1: 0.0221 \tff_accuracy_layer_1: 0.9991 \tloss_layer_2: 0.1321 \tff_accuracy_layer_2: 0.9953 \tclassification_loss: 0.0067 \tclassification_accuracy: 0.9987 \t\n",
            "Epoch 82 \ttrain \t \tTime: 0:00:19.526044 \tLoss: 0.2379 \tPeer Normalization: 0.5201 \tloss_layer_0: 0.0597 \tff_accuracy_layer_0: 0.9965 \tloss_layer_1: 0.0258 \tff_accuracy_layer_1: 0.9989 \tloss_layer_2: 0.1301 \tff_accuracy_layer_2: 0.9953 \tclassification_loss: 0.0065 \tclassification_accuracy: 0.9989 \t\n",
            "Epoch 83 \ttrain \t \tTime: 0:00:18.699509 \tLoss: 0.2146 \tPeer Normalization: 0.5193 \tloss_layer_0: 0.0548 \tff_accuracy_layer_0: 0.9970 \tloss_layer_1: 0.0268 \tff_accuracy_layer_1: 0.9988 \tloss_layer_2: 0.1109 \tff_accuracy_layer_2: 0.9958 \tclassification_loss: 0.0064 \tclassification_accuracy: 0.9988 \t\n",
            "Epoch 84 \ttrain \t \tTime: 0:00:18.657281 \tLoss: 0.2119 \tPeer Normalization: 0.5209 \tloss_layer_0: 0.0576 \tff_accuracy_layer_0: 0.9969 \tloss_layer_1: 0.0312 \tff_accuracy_layer_1: 0.9990 \tloss_layer_2: 0.1011 \tff_accuracy_layer_2: 0.9962 \tclassification_loss: 0.0064 \tclassification_accuracy: 0.9989 \t\n",
            "Epoch 85 \ttrain \t \tTime: 0:00:19.001483 \tLoss: 0.2128 \tPeer Normalization: 0.5286 \tloss_layer_0: 0.0445 \tff_accuracy_layer_0: 0.9974 \tloss_layer_1: 0.0302 \tff_accuracy_layer_1: 0.9989 \tloss_layer_2: 0.1159 \tff_accuracy_layer_2: 0.9958 \tclassification_loss: 0.0064 \tclassification_accuracy: 0.9988 \t\n",
            "Epoch 86 \ttrain \t \tTime: 0:00:18.618520 \tLoss: 0.1688 \tPeer Normalization: 0.5333 \tloss_layer_0: 0.0342 \tff_accuracy_layer_0: 0.9977 \tloss_layer_1: 0.0215 \tff_accuracy_layer_1: 0.9993 \tloss_layer_2: 0.0906 \tff_accuracy_layer_2: 0.9965 \tclassification_loss: 0.0064 \tclassification_accuracy: 0.9990 \t\n",
            "Epoch 87 \ttrain \t \tTime: 0:00:21.184821 \tLoss: 0.1625 \tPeer Normalization: 0.5315 \tloss_layer_0: 0.0397 \tff_accuracy_layer_0: 0.9977 \tloss_layer_1: 0.0162 \tff_accuracy_layer_1: 0.9995 \tloss_layer_2: 0.0844 \tff_accuracy_layer_2: 0.9967 \tclassification_loss: 0.0063 \tclassification_accuracy: 0.9989 \t\n",
            "Epoch 88 \ttrain \t \tTime: 0:00:18.999586 \tLoss: 0.1531 \tPeer Normalization: 0.5274 \tloss_layer_0: 0.0400 \tff_accuracy_layer_0: 0.9974 \tloss_layer_1: 0.0198 \tff_accuracy_layer_1: 0.9992 \tloss_layer_2: 0.0712 \tff_accuracy_layer_2: 0.9969 \tclassification_loss: 0.0062 \tclassification_accuracy: 0.9989 \t\n",
            "Epoch 89 \ttrain \t \tTime: 0:00:19.102676 \tLoss: 0.1694 \tPeer Normalization: 0.5309 \tloss_layer_0: 0.0336 \tff_accuracy_layer_0: 0.9983 \tloss_layer_1: 0.0233 \tff_accuracy_layer_1: 0.9992 \tloss_layer_2: 0.0905 \tff_accuracy_layer_2: 0.9963 \tclassification_loss: 0.0061 \tclassification_accuracy: 0.9990 \t\n",
            "Epoch 90 \ttrain \t \tTime: 0:00:18.539869 \tLoss: 0.1386 \tPeer Normalization: 0.5323 \tloss_layer_0: 0.0322 \tff_accuracy_layer_0: 0.9981 \tloss_layer_1: 0.0131 \tff_accuracy_layer_1: 0.9996 \tloss_layer_2: 0.0713 \tff_accuracy_layer_2: 0.9968 \tclassification_loss: 0.0061 \tclassification_accuracy: 0.9991 \t\n",
            "Epoch 91 \ttrain \t \tTime: 0:00:19.577320 \tLoss: 0.1436 \tPeer Normalization: 0.5320 \tloss_layer_0: 0.0324 \tff_accuracy_layer_0: 0.9980 \tloss_layer_1: 0.0177 \tff_accuracy_layer_1: 0.9992 \tloss_layer_2: 0.0716 \tff_accuracy_layer_2: 0.9968 \tclassification_loss: 0.0061 \tclassification_accuracy: 0.9991 \t\n",
            "Epoch 92 \ttrain \t \tTime: 0:00:18.314741 \tLoss: 0.1220 \tPeer Normalization: 0.5312 \tloss_layer_0: 0.0261 \tff_accuracy_layer_0: 0.9982 \tloss_layer_1: 0.0123 \tff_accuracy_layer_1: 0.9995 \tloss_layer_2: 0.0615 \tff_accuracy_layer_2: 0.9973 \tclassification_loss: 0.0062 \tclassification_accuracy: 0.9991 \t\n",
            "Epoch 93 \ttrain \t \tTime: 0:00:19.748658 \tLoss: 0.1204 \tPeer Normalization: 0.5317 \tloss_layer_0: 0.0263 \tff_accuracy_layer_0: 0.9985 \tloss_layer_1: 0.0113 \tff_accuracy_layer_1: 0.9995 \tloss_layer_2: 0.0609 \tff_accuracy_layer_2: 0.9971 \tclassification_loss: 0.0060 \tclassification_accuracy: 0.9991 \t\n",
            "Epoch 94 \ttrain \t \tTime: 0:00:19.269620 \tLoss: 0.1105 \tPeer Normalization: 0.5309 \tloss_layer_0: 0.0240 \tff_accuracy_layer_0: 0.9986 \tloss_layer_1: 0.0161 \tff_accuracy_layer_1: 0.9995 \tloss_layer_2: 0.0485 \tff_accuracy_layer_2: 0.9977 \tclassification_loss: 0.0060 \tclassification_accuracy: 0.9991 \t\n",
            "Epoch 95 \ttrain \t \tTime: 0:00:19.382331 \tLoss: 0.0965 \tPeer Normalization: 0.5312 \tloss_layer_0: 0.0189 \tff_accuracy_layer_0: 0.9989 \tloss_layer_1: 0.0130 \tff_accuracy_layer_1: 0.9996 \tloss_layer_2: 0.0426 \tff_accuracy_layer_2: 0.9980 \tclassification_loss: 0.0061 \tclassification_accuracy: 0.9991 \t\n",
            "Epoch 96 \ttrain \t \tTime: 0:00:18.414770 \tLoss: 0.0961 \tPeer Normalization: 0.5313 \tloss_layer_0: 0.0190 \tff_accuracy_layer_0: 0.9990 \tloss_layer_1: 0.0117 \tff_accuracy_layer_1: 0.9996 \tloss_layer_2: 0.0435 \tff_accuracy_layer_2: 0.9976 \tclassification_loss: 0.0059 \tclassification_accuracy: 0.9991 \t\n",
            "Epoch 97 \ttrain \t \tTime: 0:00:19.641899 \tLoss: 0.0879 \tPeer Normalization: 0.5319 \tloss_layer_0: 0.0160 \tff_accuracy_layer_0: 0.9989 \tloss_layer_1: 0.0073 \tff_accuracy_layer_1: 0.9997 \tloss_layer_2: 0.0427 \tff_accuracy_layer_2: 0.9979 \tclassification_loss: 0.0059 \tclassification_accuracy: 0.9991 \t\n",
            "Epoch 98 \ttrain \t \tTime: 0:00:18.567923 \tLoss: 0.0769 \tPeer Normalization: 0.5305 \tloss_layer_0: 0.0159 \tff_accuracy_layer_0: 0.9991 \tloss_layer_1: 0.0052 \tff_accuracy_layer_1: 0.9998 \tloss_layer_2: 0.0340 \tff_accuracy_layer_2: 0.9981 \tclassification_loss: 0.0059 \tclassification_accuracy: 0.9992 \t\n",
            "Epoch 99 \ttrain \t \tTime: 0:00:19.489362 \tLoss: 0.0852 \tPeer Normalization: 0.5309 \tloss_layer_0: 0.0235 \tff_accuracy_layer_0: 0.9991 \tloss_layer_1: 0.0080 \tff_accuracy_layer_1: 0.9998 \tloss_layer_2: 0.0320 \tff_accuracy_layer_2: 0.9983 \tclassification_loss: 0.0058 \tclassification_accuracy: 0.9992 \t\n",
            "val\n",
            "val \t \tTime: 0:00:03.146549 \tLoss: 0.0680 \tclassification_loss: 0.0680 \tclassification_accuracy: 0.9824 \t\n",
            "test\n",
            "test \t \tTime: 0:00:03.126868 \tLoss: 0.0544 \tclassification_loss: 0.0544 \tclassification_accuracy: 0.9837 \t\n"
          ]
        }
      ]
    }
  ]
}